# -*- coding: utf-8 -*-
"""finalize diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8988shCSBjDXMHTMz5_Dr1Y3na6gxfA
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
# !pip install missingno
import missingno as msno
from datetime import date
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
# pd.set_option('display.width', 500)
from sklearn.metrics import classification_report  #classification report mainly used for comparing the value of predicted output and actual output
from sklearn.metrics import confusion_matrix, accuracy_score , recall_score , precision_score ,f1_score

import os

# !pip3 install catboost
from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
import xgboost as xgb
from xgboost import XGBClassifier
import lightgbm as lgb
from lightgbm import LGBMClassifier

from catboost import CatBoostClassifier

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.metrics import confusion_matrix, accuracy_score , recall_score , precision_score

from google.colab import drive
drive.mount('/content/drive')

df_= pd.read_csv("/content/drive/MyDrive/datasets/diabetes.csv")
df = df_.copy()


df.columns = [col.upper() for col in df.columns]
df.head()

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    # print(f"Observations: {dataframe.shape[0]}")
    # print(f"Total feature or Variables: {dataframe.shape[1]}")
    # print(f'cat_cols: {len(cat_cols)}')
    # print(f'num_cols: {len(num_cols)}')
    # print(f'num_but_cat: {len(num_but_cat)}')
    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = grab_col_names(df)

num_cols

def check_df(dataframe, head=5):
    print("The shape of dataframe:")
    print(dataframe.shape)
    print("The types of feature of dataframe:")
    print(dataframe.dtypes)

    print(dataframe.head(head))

    print("Checks Null any")
    print(dataframe.isnull().sum())

check_df(df)

print(df.isnull().sum())

def target_summary_with_num(dataframe, target, numerical_col):
    print(dataframe.groupby(target).agg({numerical_col: "median"}), end="\n\n\n")
for col in num_cols:
    target_summary_with_num(df,"OUTCOME", col)



df.isnull().sum() # No more missing Values or wrong values

df.describe().T

fig, axes = plt.subplots(2, 4, figsize=(24, 10))
for i, col in enumerate(num_cols):
    sns.boxplot(x=df[col], ax=axes[i//4, i%4])
    axes[i//4, i%4].set_title(col)
plt.show(block=True)

"""# Feature Engineering"""

def feature_engineering(df):
  # Glucose
# BloodPressure
# SkinThickness
# Insulin
# BMI :Based on these figures a mean BMI of 12 as the lower limit for human survival emerges - a value first proposed by James et al (1988)
# BMI lower than 12 will also be NaN
# 90/60mmHg or less is considered the lowest blood pressure before death.


  df.loc[df["BLOODPRESSURE"]<60, "BLOODPRESSURE"].count()      #121
  df.loc[df["BLOODPRESSURE"]<60, "BLOODPRESSURE"] = 0 # I changed all the values under 60 with 0 (since they are wrong values)

  df[df["BMI"]<12]['BMI'].count() #11
  df.loc[df["BMI"]<12,"BMI"] = 0

  cat_cols, num_cols, cat_but_car = grab_col_names(df)
  for col in [col for col in num_cols if col not in "PREGNANCIES"]:
     df[col].replace(0, np.nan, inplace=True)
  return df


# df=feature_engineering(df)
# df.head()

"""# Missing value Handle"""

def missing_value_Fillup(df):
  cat_cols, num_cols, cat_but_car = grab_col_names(df)
  for col in [col for col in num_cols if col not in "PREGNANCIES"]:
     df[col].fillna(df.groupby("OUTCOME")[col].transform("median"), inplace = True)

  return df


# df = missing_value_Fillup(df)
# df.head()



#@title Outlier Analysis
def outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

check_outlier(df, num_cols)

#@title Handling Outliers
def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit


def handle_outliers(df):
  cat_cols, num_cols, cat_but_car = grab_col_names(df)
  for col in num_cols:
    replace_with_thresholds(df,col)
  return df


# df = handle_outliers(df)
# df.head()

#@title Categorical and Numerical Analysis
def cat_summary(dataframe, col_name, plot=False):
    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
                        "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))
    print("##########################################")

    if plot:
        sns.countplot(x=dataframe[col_name], data=dataframe)
        plt.show(block=True)
def target_summary_with_cat(dataframe, target, categorical_col):
    print(pd.DataFrame({"TARGET_MEAN": dataframe.groupby(categorical_col)[target].mean()}), end="\n\n\n")
def num_summary(dataframe, numerical_col, plot=False):
    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
    print(dataframe[numerical_col].describe(quantiles).T)

    if plot:
        dataframe[numerical_col].hist()
        plt.xlabel(numerical_col)
        plt.title(numerical_col)
        plt.show(block=True)
def target_summary_with_num(dataframe, target, numerical_col):
    print(dataframe.groupby(target).agg({numerical_col: "mean"}), end="\n\n\n")

for col in cat_cols:
    cat_summary(df, col, plot=True)

for col in num_cols:
    num_summary(df, col, plot=True)

#@title Correlation

plt.figure(dpi = 120,figsize= (5,4))
mask = np.triu(np.ones_like(df.corr(),dtype = bool))
sns.heatmap(df.corr(),mask = mask, fmt = ".2f",annot=True,lw=1,cmap = 'plasma')
plt.yticks(rotation = 0)
plt.xticks(rotation = 90)
plt.title('Correlation Heatmap')
plt.show(block=True)

#@title Feature Extraction
def binning(df):
  df['GLUCOSE_CAT'] = pd.cut(x = df['GLUCOSE'], bins = [-1,80,140,160,200],
                               labels = ['Hypoglecimia','Normal', 'Impaired_Glucose', 'Diabetic_Glucose'])

  df['AGE_CAT']= pd.cut(df['AGE'], bins = [0, 20, 30 ,50, 81], labels=['Young','Young Adult', 'Adult', 'Old'])

  df.loc[(df['PREGNANCIES'] == 0), 'PREGNANT_CAT']  = 'Never'
  df.loc[(df['PREGNANCIES'] == 1), 'PREGNANT_CAT']  = 'One Time'
  df.loc[(df['PREGNANCIES'] > 1), 'PREGNANT_CAT']   = 'Many Times'

  return df

# df=binning(df)
# df.head()

#@title Feature Generating
def feature_generating(df):
  df["AGE*BMI_NEW"] = df["AGE"] * df["BMI"]

  df["NEW_GLUCOSE*INSULIN"] = df["GLUCOSE"] * df["INSULIN"]
  return df

# df=feature_generating(df)
# df.head()

df[num_cols].head()

#@title Feature Scaling
def Normalization(df):
  rs= RobustScaler()
  cat_cols, num_cols, cat_but_car = grab_col_names(df)
  df[num_cols]= rs.fit_transform(df[num_cols])
  return df

# df=Normalization(df)
# df.head()

#@title Label Encoding
def one_hot_encoder( df,drop_first=True, dummy_na =False):
    categorical_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]
    dataframe = pd.get_dummies(df, columns=categorical_cols, drop_first=drop_first, dummy_na=dummy_na)
    return dataframe

# df = one_hot_encoder(df)
# df.head()

"""# Pipelining preprocessing steps"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer



# Create the pipeline with FunctionTransformer for each step
preprocessing_pipeline = Pipeline([
    ('feature_engineering', FunctionTransformer(feature_engineering, validate=False)),
    ('missing_value', FunctionTransformer(missing_value_Fillup, validate=False)),
    ('outlier_handler', FunctionTransformer(handle_outliers, validate=False)),

    ('binning', FunctionTransformer(binning, validate=False)),
    ('feature_generation', FunctionTransformer(feature_generating, validate=False)),
    ('normalization', FunctionTransformer(Normalization, validate=False)),
    ('one_hot_encoding', FunctionTransformer(one_hot_encoder, validate=False)),
])

# Apply the pipeline to your DataFrame
processed_df = preprocessing_pipeline.fit_transform(df)
print(processed_df.head())

"""#  train test split"""

#@ title
y = processed_df["OUTCOME"]
X = processed_df.drop(["OUTCOME"], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)

y.value_counts()

"""# Smote(handle imbalance dataset)"""

#Using SMOTE to balance the data
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_train, y_train = smote.fit_resample(X_train, y_train)
ros_chd_plot=y_train.value_counts().plot(kind='bar')
plt.show()

"""# Neural network model"""

import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l1
from keras.callbacks import EarlyStopping

# Define the model
model1 = Sequential()
model1.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l1(0.001)))
model1.add(Dense(32, activation='relu', kernel_regularizer=l1(0.001)))

model1.add(Dense(1, activation='sigmoid'))

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)

# Compile the model
model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model1.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model1.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
print(model1.predict(X_test).shape)

y_pred1=model1.predict(X_test)
y_pred=(y_pred1>0.5).astype("int32")



"""# Multiple Machine Learning model training (CROSS VALIDATION AND GRID SEARCH CV)"""

# Create a list of models
models = [LogisticRegression(), SVC(), KNeighborsClassifier(n_neighbors=6),
          DecisionTreeClassifier(max_leaf_nodes=3, random_state=0, criterion='entropy'),
          RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1),
          XGBClassifier(), GaussianNB(), SGDClassifier(),
          RidgeClassifier(), BaggingClassifier(), LGBMClassifier(random_state=0),
          CatBoostClassifier(verbose=False, random_state=0),
          ]
# List of model names
model_names = ["LogisticRegression", "SVC", "KNeighborsClassifier", "DecisionTreeClassifier", "RandomForestClassifier", "XGBClassifier", "GaussianNB", "SGDClassifier", "RidgeClassifier", "BaggingClassifier", "LGBMClassifier", "CatBoostClassifier"]
# Create a list of dictionaries for the hyperparameters to be tuned for each model


param_grids = [
    # Logistic Regression
      {'penalty': ['l2'], 'C': [0.1, 1, 10]},

    # Support Vector Machine (SVC)
    {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']},

    # K-Nearest Neighbors (KNN)
    {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},

    # Decision Tree Classifier
    {'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 10, 15], 'min_samples_split': [2, 5, 10]},

    # Random Forest Classifier
    {'n_estimators': [200, 300], 'max_depth': [None, 5], 'min_samples_split': [2, 5], 'max_features': [0.3, 0.5]},

    # XGBoost Classifier
    {'eta': [0.01, 0.1, 0.3], 'max_depth': [3, 6, 9], 'subsample': [0.7, 0.8, 0.9], 'colsample_bytree': [0.7, 0.8, 0.9]},

    # Gaussian Naive Bayes
    {},

    # Stochastic Gradient Descent (SGD) Classifier
    {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l1', 'l2', 'elasticnet']},

    # Ridge Classifier
    {'alpha': [0.1, 0.5, 1.0], 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']},

    # Bagging Classifier
    {'n_estimators': [50, 100, 150], 'max_samples': [0.5, 0.7, 0.9], 'max_features': [0.5, 0.7, 0.9]},

    # LightGBM Classifier
    {'num_leaves': [10, 20, 30], 'learning_rate': [0.01, 0.05, 0.1, 0.3], 'max_depth': [-1, 5, 10]},

    # CatBoost Classifier
    {'iterations': [100, 150, 200], 'learning_rate': [0.01, 0.05, 0.1], 'depth': [4, 6, 8]}
]

best_params_dict = {}
best_models = []
# Loop through each model and perform GridSearchCV
for i, model in enumerate(models):
    model_name = model_names[i]
    print("Tuning hyperparameters for Model", model_name)
    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[i], cv=5)
    grid_search.fit(X_train, y_train)  # Replace X_train and y_train with your training data
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    print("Best parameters:", best_params)
    print("Best score:", best_score)
    print()

    # Store the best parameters in the dictionary

    best_params_dict[model_name] = best_params

    # Create a new instance of the model with the best parameters and add it to the best_models list
    if model_name == 'CatBoostClassifier':
      best_model = model.__class__(**best_params, verbose =0)
    else:
      best_model = model.__class__(**best_params)
    best_models.append(best_model)



for model, model_name in zip(best_models, model_names):
  print(model_name)
  if model_name == 'CatBoostClassifier':

      model.fit(X_train, y_train, verbose =0) #fit those best model
  else:

      model.fit(X_train, y_train) #fit those best model



"""# EVALUATION METRICES FOR ALL MODEL"""

from sklearn.metrics import confusion_matrix


# Iterate over models
for model, model_name in zip(best_models, model_names):
    # Assuming you have test data and predictions for each model
    y_pred = model.predict(X_test)

    # Compute confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Print confusion matrix
    print("Evaluation metrices for", model_name)
    print(classification_report(y_test, y_pred))
    print()
    print()
    print()
    print()

"""# Confusion Matrix

---


"""

# Compute confusion matrix

def plot_cm(cm, model_name):
  # Define class labels (replace with your own if necessary)
  class_labels = ["Heart patient", "Normal patient",]

  # Plot confusion matrix
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
  #plt.title("Confusion Matrix for ", model_name)
  plt.xlabel("Predicted Labels")
  plt.ylabel("True Labels")
  plt.show()

from sklearn.metrics import confusion_matrix

# Iterate over models
for model, model_name in zip(best_models, model_names):
    # Assuming you have test data and predictions for each model
    y_pred = model.predict(X_test)

    # Compute confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Print confusion matrix
    print("Confusion Matrix for", model_name)
    print(cm)

    print()
    plot_cm(cm,model_name)
    print()

"""# ROC Curve"""

def roc_plot(y_pred):
      # Compute the false positive rate, true positive rate, and thresholds for ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)

    # Compute the area under the ROC curve
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) - ' + model_name)
    plt.legend(loc="lower right")
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np


# Iterate over models
for model, model_name in zip(best_models, model_names):
    # Get the corresponding prediction for the model
    y_pred = model.predict(X_test)

    roc_plot(y_pred)

    print()
    print()
    print()
    print()



"""# Learning curve"""

def plot_learning_curve(model):
      # Compute learning curve
    if model_name == 'CatBoostClassifier':

      train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5,verbose = 0)

    else:
      train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Plot learning curve
    plt.figure()
    plt.title('Learning Curve - ' + model_name)
    plt.xlabel("Training Examples")
    plt.ylabel("Accuracy Score")
    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training Score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation Score")
    plt.legend(loc="best")
    plt.show()



from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np


# Iterate over models
for model, model_name in zip(best_models, model_names):
    plot_learning_curve(model)





"""# Feature Importance"""

def plot_importance(model, features, num=len(X)):
    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})
    plt.figure(figsize = (10, 10))
    sns.set(font_scale = 1)
    sns.barplot(x = "Value", y = "Feature", data = feature_imp.sort_values(by = "Value", ascending = False)[0:num])
    plt.title('Features')
    plt.tight_layout()
    plt.show(block=True)
plot_importance(best_models[4], X_train)  # show the feature importance generated by random forest as it shows better result

"""# Model Interpretation (SHAP)"""

# !pip install shap

import shap
import matplotlib
explainer = shap.TreeExplainer(best_models[4])

# calculate shap values. This is what we will plot.
shap_values = explainer.shap_values(X_test)

# custom colour plot
colors = ["#9bb7d4", "#0f4c81"]
cmap = matplotlib.colors.LinearSegmentedColormap.from_list("", colors)
shap.summary_plot(shap_values[1], X_test,cmap=cmap,alpha=0.4)

shap.dependence_plot('INSULIN', shap_values[1], X_test, interaction_index="INSULIN",cmap=cmap,alpha=0.4,show=False)
plt.title("Age dependence plot",loc='left',fontfamily='serif',fontsize=15)
plt.ylabel("SHAP value for the 'INSULIN' feature")
plt.show()



def model_evaluation(model,model_name,y_pred):
    #Evaluation metrices
    print("Evaluation metrices for", model_name)
    print(classification_report(y_test, y_pred))
    print()

    # Print confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix for", model_name)
    print(cm)

    print()
    plot_cm(cm,model_name)
    print()

    #plot the roc curve
    roc_plot(y_pred)
    print()
    print()

    #plot the learning curve
    plot_learning_curve(model)
    print()
    print()

"""# Voting Classifier"""



from sklearn.ensemble import StackingClassifier
from sklearn.svm import NuSVC

#soft voting classifier
from sklearn.ensemble import VotingClassifier

estimator = []
# Indices of models to be used as voting estimators
voting_indices = [4, 5, 9, 10, 11]
for i in voting_indices:
  estimator.append((model_names[i],best_models[i]))

# Create a voting classifier with 'soft' voting using the selected models
voting_classifier = VotingClassifier(estimators=estimator, voting='soft')

# Train and evaluate the voting classifier
voting_classifier.fit(X_train, y_train)
y_pred = voting_classifier.predict(X_test)

model_evaluation(voting_classifier,'voting classifier soft',y_pred)

#hard voting classifier

from sklearn.ensemble import VotingClassifier


estimator = []
# Indices of models to be used as voting estimators
voting_indices = [4, 5, 9, 10, 11]
for i in voting_indices:
  estimator.append((model_names[i],best_models[i]))

# Create a voting classifier with 'soft' voting using the selected models
voting_classifier = VotingClassifier(estimators=estimator, voting='hard')

# Train and evaluate the voting classifier
voting_classifier.fit(X_train, y_train)
y_pred = voting_classifier.predict(X_test)

model_evaluation(voting_classifier,'voting classifier hard',y_pred)

"""# StackingClassifier"""



from sklearn.ensemble import StackingClassifier
from sklearn.metrics import classification_report

estimator = []
# Indices of models to be used as voting estimators
stacking_indices = [4, 5, 9, 10, 11]
for i in stacking_indices:
  estimator.append((model_names[i],best_models[i]))

# Create the stacking classifier with 'soft' voting
stacking_classifier = StackingClassifier(estimators=estimator,  final_estimator=NuSVC())

# Train and evaluate the stacking classifier
stacking_classifier.fit(X_train, y_train)
y_pred = stacking_classifier.predict(X_test)

model_evaluation(stacking_classifier,'stacking classifier soft',y_pred)

# Define the hyperparameters to tune
param_grid = {
    'final_estimator__nu': [0.3, 0.5, 0.7],
    'final_estimator__kernel': ['linear', 'rbf', 'sigmoid']
}

# Perform grid search for hyperparameter tuning
stacking_grid_search = GridSearchCV(estimator=stacking_classifier, param_grid=param_grid, cv=5)
stacking_grid_search.fit(X_train, y_train)

# Get the best model
best_model = stacking_grid_search.best_estimator_

# Evaluate the best model on the test set
accuracy = best_model.score(X_test, y_test)
print("Accuracy:", accuracy)

